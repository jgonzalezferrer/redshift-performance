{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Redshift credentials.\n",
    "with open('credentials.yml') as f:\n",
    "    conf = yaml.load(f)\n",
    "    \n",
    "DBNAME = conf['login']['dbname']\n",
    "HOST = conf['login']['host']\n",
    "PORT = conf['login']['port']\n",
    "USER = conf['login']['user']\n",
    "PASSWORD = conf['login']['password']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dc2.large\n",
    "slices_per_node = 2\n",
    "nodes = 2\n",
    "slices = slices_per_node * nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection\n",
    "con = psycopg2.connect(dbname=DBNAME, host=HOST, port=PORT, user=USER, password=PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a cursor to perform database operations\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "* check the cluster configuration (nodes and slices)\n",
    "* load data from parquet vs. csv vs. compressed data files (e.g. gzip)?\n",
    "* load data copy vs. insert\n",
    "* select operations defined sort key vs undefined sort key\n",
    "* join operations defined distkey vs undefined distkey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "1. Create table\n",
    "2. Create several files and upload to s3\n",
    "3. Copy\n",
    "\n",
    "TO CHECK:\n",
    "* Manifest file to explicitly specify files (also guarantees consistency)\n",
    "* Automatic Compression\n",
    "* Optimizing Storage for Narrow Tables (due to hidden vars)\n",
    "* update and insert new data -> and then vacuum\n",
    "* UNLOAD data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create mock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "n_rows = 5*10**5\n",
    "n_columns = 100\n",
    "column_names = list(range(n_columns))\n",
    "column_names = [str(c) for c in column_names]\n",
    "\n",
    "df = pd.DataFrame(np.random.randint(0, 100, size=(n_rows, n_columns)), columns=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load table data from a single file, or you can split the data for each table into multiple files. The COPY command can load data from multiple files in parallel. You can load multiple files by specifying a common prefix, or prefix key, for the set, or by explicitly listing the files in a manifest file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/single_numbers.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to_csv (compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/single_numbers.gz', index=False, header=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('data/single_numbers.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to_csv (multiple files)\n",
    "Amazon Redshift does not take file size into account when dividing the workload, so you need to ensure that the files are roughly the same size, between 1 MB and 1 GB after compression.\n",
    "\n",
    "Split your data into files so that the number of files is a multiple of the number of slices in your cluster. That way Amazon Redshift can divide the data evenly among the slices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_chunks = slices * 25\n",
    "for chunk, df_i in enumerate(np.array_split(df, number_of_chunks)):\n",
    "    path = 'data/split_numbers.csv.{}'.format(chunk)\n",
    "    df_i.to_csv(path, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to_parquet (multiple files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk, df_i in enumerate(np.array_split(df, number_of_chunks)):\n",
    "    path = 'data/split_numbers.parquet.{}'.format(chunk)\n",
    "    df_i.to_parquet(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3.Bucket(name='redshift-performance-data')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AWS_ACCESS_KEY_ID = conf['aws_access']['key_id']\n",
    "AWS_SECRET_ACCESS_KEY = conf['aws_access']['secret_key']\n",
    "bucket_name = 'redshift-performance-data'\n",
    "\n",
    "s3 = boto3.resource('s3', aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "s3.create_bucket(Bucket=bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir('data'):\n",
    "        s3.Object(bucket_name, filename).put(Body=open(os.path.join('data', filename), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
